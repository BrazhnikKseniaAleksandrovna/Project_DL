# Распознавание ключевых голосвых команд
Проект, направленный на обнаружение однословных команд на основе набора данных Google Speech Commands. Мы извлекли из него всего 10 слов (их в наборе данных 35), чтобы сократить время обучения.

# О наборе данных
Google Speech Commands — это открытый датасет коротких голосовых команд на английском языке, разработанный Google для обучения моделей распознавания речи. Набор данных содержит 105 829 аудиофайлов продолжительностью в одну секунду. Каждый клип содержит одно из 35 различных слов (например: "yes", "no", "stop", "go", "left", "right" и т.д.), произнесенных тысячами разных субъектов.  

Мы ограничили количество классов для ускорения экспериментов, выбрав 10 основных команд:
- Основные команды: 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go'
- Дополнительно мы добавили класс 'unknown' для всех остальных команд, чтобы модель могла распознавать посторонние слова

Для балансировки данных мы:
- Сохранили все примеры выбранных классов
- Для класса 'unknown' взяли только 4% от общего количества примеров 
- Равномерно распредели примеры между разными словами в классе 'unknown'
на трейне, получили такой распределение:

 1. down: 3134
 2. go: 3106
 3. left: 3037
 4. no: 3130
 5. off: 2970
 6. on: 3086
 7. right: 3019
 8. stop: 3111
 9. up: 2948
 10. yes: 3228
 11. unknown: 3375
 
Классы у нас хорошо расспределены, нет дисбаланса.

# Пару слов об аугментации
Для повышения устойчивости модели к реальным условиям мы использовали комплекс аугментаций, которые делают данные более разнообразными и приближенными к реальным записям. Основные преобразования включают добавление случайного шума, временной сдвиг и изменение громкости.  

Добавление шума имитирует фоновые помехи — мы генерировали гауссовский шум с амплитудой 0.003 и накладывали его на исходный сигнал. Это помогает модели научиться игнорировать слабые шумы, такие как фоновые разговоры или помехи микрофона, делая её более устойчивой к неидеальным условиям записи.  

Временной сдвиг позволяет имитировать естественные вариации в начале и конце произнесения команд. Мы случайным образом смещали аудиосигнал в пределах ±150 миллисекунд, обрезая излишки и дополняя нулями. Это важно, потому что в реальном времени команды могут начинаться не строго в один и тот же момент, и модель должна быть к этому готова.  

Изменение громкости помогает учесть различия в расстоянии до микрофона и громкости голоса. Мы умножаем сигнал на случайный коэффициент в диапазоне от 0.7 до 1.3, сохраняя разборчивость, но при этом заставляя модель адаптироваться к разной амплитуде звука.  

# Предобработка данных
Каждый аудиофайл проходит через строгий pipeline преобразований:

а) Конвертация в моно-формат
   - Все стереозаписи преобразуются в моно путем усреднения каналов
   - Это уменьшает размерность данных без потери ключевой информации

б) Ресемплирование
   - Все записи приводятся к единой частоте 16 кГц
   - Используется билинейная интерполяция для сохранения качества

в) Нормализация длины
   - Фиксированная длительность 1 секунда (16000 samples)
   - Короткие записи дополняются нулями (zero-padding)
   - Длинные записи обрезаются случайным образом

г) Преобразование в мел-спектрограмму
   - Применяется преобразование Фурье с окном 1024 samples
   - Используется 64 мел-фильтра для получения спектральных характеристик
   - Шаг окна (hop_length) 256 samples обеспечивает хороший компромисс между детализацией и размером данных

д) Логарифмическое преобразование
   - Амплитуды преобразуются в децибелы для лучшего восприятия моделью
   - Проводится стандартная нормализация.

# Обучение модели

Обучали модель на своих данных: CNN + FCNN

![Архитектура нейронной сети](https://github.com/user-attachments/assets/9faa415a-b731-49b4-a3e3-b8967a07c62f)

CNN-блок:
- 2 сверточных слоя с ядром 3×3 и ReLU-активацией; 
- 2 слоя субдискретизации (MaxPooling) работает с входами формата [batch, 1, mel, time].
- GAP (Global Average Pooling) — nn.AdaptiveAvgPool2d((1, 1)) — приводит карту признаков к фиксированному размеру независимо от входа.

Полносвязный блок:
- слой Flatten;
- линейный слой 16→64;
- ReLU;
- линейный слой 64→количество классов.

Метрику использовали базовую: accuracy

# Итог
По итогу класс unknown мешал обучению, что приводило к низкому качеству модели. Мы решили исключить его, после чего модель стала предсказывать правильные команды, произнесенные нами. Аугментация также оказала положительный эффект на обучение, точность на тестовой выборке выросла на 1-2 процента. (Итоговый файл - Voice_augnun.ipynb)

# Как развернуть веб сервис + приложение

 !!! Запускать локально, так как есть вероятность столкнуться с ограничениями записи с микрофона
 
 !!! Потребуется VPN

**Что делать для веб сервиса**

 1) Скачать файл speech_command_cnn.pth, label2idx.pkl, web_service.ipynb и app.py
 2) Запустить код в файле web_service.ipynb
 3) Перейти по ссылке, которая выведится в предпоследнем блоке
 4) Вас должен ожидать такой сайт:
![image](https://github.com/user-attachments/assets/32d186aa-1a5b-4810-a206-03f105a92274)

**Что делать для приложения**
 1) Скачать файл voice_app.ipynb 
 2) Запустить код в файле voice_app.ipynb 
 3) Перейти по ссылке, которая выведится в предпоследнем блоке
 4) Вас должен ожидать такой милый котик:
![image](https://github.com/user-attachments/assets/37cf1816-d1f5-451d-a529-0421ef74a468)


P.S. Если возникнут вопросы с получением ссылки, то обратитесь к нам




