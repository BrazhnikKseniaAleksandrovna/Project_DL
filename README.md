# Project_DL
Проект на тему: “Распознавание мелкоречевых оборотов (команды, жаргонизмы и тд)”

Мы решили сузить нашу тему до распознавание команд. (Изначально мы рассматривали распознавание военных команд, но из-за отсутствия открытых датасетов) сосредоточились на простых командах. В качестве базы данных использовали Google Speech Command.

Google Speech Commands — это открытый датасет коротких голосовых команд, разработанный Google для обучения моделей распознавания речи. Представляет из себя набор односекундных .wav аудиофайлов, каждый из которых содержит одно произнесенное английское слово или фоновый шум. Эти слова взяты из небольшого набора команд и произносятся разными дикторами. 

Мы взяли версию 0.02 набора данных (конфигурация "v0.02"), которая была выпущена 11 апреля 2018 года и содержит 105 829 аудиофайлов.
Язык: английский.
Датасет включает: 35 базовых слов (например: "yes", "no", "stop", "go", "left", "right" и т.д.), 10 цифр ("zero", "one", ..., "nine"), а так же фоновые шумы (для улучшения устойчивости моделей)

Метрику использовали базовую: accuracy

Прочитав статью "Конволюционные нейронные сети для поиска ключевых словна малом пространстве" (Tara N. Sainath, Carolina Parada), мы решили использовать именно Сверточные Нейронные Сети (CNN), так как они:
- Эффективны для задач распознавания коротких аудиокоманд.
- Превосходят DNN по точности при меньшем количестве параметров (улучшение на 27–44% по метрике False Rejection Rate).
- Оптимизированы для работы на устройствах с ограниченными ресурсами.

Попробов архитектуры, предложенные в статье, например:
  1. cnn-trad-fpool3: Два свёрточных слоя с пулингом по частоте.
  2. cnn-one-fstride4: Один свёрточный слой со страйдингом для экономии вычислений.

Мы решили также хотели рассмотреть предобученные модели(superb/wav2vec2-base-superb-ks, facebook/wav2vec2-base-960h), но, к сожалению, из-за неустойчивой работы datasphera и проблемой с библиотеками, у нас не получилось узнать лучше ли они, чем наши. Поэтому оставили свои.

В Итоге мы составили свою архитектуру модели. 
Модель построена на основе сверточной нейросети (CNN) и используется для классификации коротких аудиофрагментов:

CNN-блок (self.cnn):

2 сверточных слоя с ядром 3×3 и ReLU-активацией;
2 слоя субдискретизации (MaxPooling);
работает с входами формата [batch, 1, mel, time].
GAP (Global Average Pooling) — nn.AdaptiveAvgPool2d((1, 1)) — приводит карту признаков к фиксированному размеру независимо от входа.

Полносвязный блок (self.fc):

слой Flatten;
линейный слой 16→64;
ReLU;
линейный слой 64→количество классов.
Метод forward(x):

добавляет канал (unsqueeze);
последовательно применяет все блоки.

Теперь расскажем об обработке: 

Мы ограничили количество классов для ускорения экспериментов, выбрав 10 основных команд:
- Основные команды: 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go'
- Дополнительно мы добавили класс 'unknown' для всех остальных команд, чтобы модель могла распознавать посторонние слова

Для балансировки данных мы:
- Сохранили все примеры выбранных классов
- Для класса 'unknown' взяли только 4% от общего количества примеров 
- Равномерно распредели примеры между разными словами в классе 'unknown'
на трейне, получили такой распределение:
down: 3134
go: 3106
left: 3037
no: 3130
off: 2970
on: 3086
right: 3019
stop: 3111
up: 2948
yes: 3228
unknown: 3375
Классы у нас хорошо расспределены, нет дисбаланса. 
  (если есть желание усложнить задачу, можно добавить forward, backward, follow, но мы не нашли применение этих команд в нашем приложении, а так же эти в этих классах меньше данных, чем в остальных, раза в 2)
А также каждый аудиофайл проходит через строгий pipeline преобразований:

а) Конвертация в моно-формат
- Все стереозаписи преобразуются в моно путем усреднения каналов
- Это уменьшает размерность данных без потери ключевой информации

б) Ресемплирование
- Все записи приводятся к единой частоте 16 кГц
- Используется билинейная интерполяция для сохранения качества

в) Нормализация длины
- Фиксированная длительность 1 секунда (16000 samples)
- Короткие записи дополняются нулями (zero-padding)
- Длинные записи обрезаются случайным образом

г) Преобразование в мел-спектрограмму
- Применяется преобразование Фурье с окном 1024 samples
- Используется 64 мел-фильтра для получения спектральных характеристик
- Шаг окна (hop_length) 256 samples обеспечивает хороший компромисс между детализацией и размером данных

д) Логарифмическое преобразование
- Амплитуды преобразуются в децибелы для лучшего восприятия моделью
- Проводится стандартная нормализация.

Для повышения устойчивости модели к реальным условиям мы использовали комплекс аугментаций, которые делают данные более разнообразными и приближенными к реальным записям. Основные преобразования включают добавление случайного шума, временной сдвиг и изменение громкости.  

Добавление шума имитирует фоновые помехи — мы генерировали гауссовский шум с амплитудой 0.003 и накладывали его на исходный сигнал. Это помогает модели научиться игнорировать слабые шумы, такие как фоновые разговоры или помехи микрофона, делая её более устойчивой к неидеальным условиям записи.  

Временной сдвиг позволяет имитировать естественные вариации в начале и конце произнесения команд. Мы случайным образом смещали аудиосигнал в пределах ±150 миллисекунд, обрезая излишки и дополняя нулями. Это важно, потому что в реальном времени команды могут начинаться не строго в один и тот же момент, и модель должна быть к этому готова.  

Изменение громкости помогает учесть различия в расстоянии до микрофона и громкости голоса. Мы умножаем сигнал на случайный коэффициент в диапазоне от 0.7 до 1.3, сохраняя разборчивость, но при этом заставляя модель адаптироваться к разной амплитуде звука.  

НО, к сожалению, приложение стало хуже работать с аугментацией, а так же видимо, наша задумака с добавлением класса unknown провалилась.
