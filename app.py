import streamlit as st
import torch
import torchaudio
import torchaudio.transforms as T
import torch.nn as nn
import pickle
import io
import numpy as np
import tempfile
from pydub import AudioSegment

st.set_page_config(page_title="–ì–æ–ª–æ—Å–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã", layout="centered")
st.title("üé§ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥ –∏–∑ —Ñ–∞–π–ª–∞")

SAMPLE_CLASSES = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']

class SpeechCommandModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.gap = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(16, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        x = self.cnn(x)
        x = self.gap(x)
        x = self.fc(x)
        return x

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
device = torch.device('cpu')
model = SpeechCommandModel(num_classes=len(SAMPLE_CLASSES))
model.load_state_dict(torch.load("speech_command_cnn.pth", map_location=device))
model.eval()

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –º–µ—Ç–æ–∫
with open("label2idx.pkl", "rb") as f:
    label2idx = pickle.load(f)
idx2label = {v: k for k, v in label2idx.items()}

st.markdown("""
#### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:
1. –ó–∞–ø–∏—à–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É —á–µ—Ä–µ–∑ [online-voice-recorder.com/ru](https://online-voice-recorder.com/ru) –∏–ª–∏ –ª—é–±–æ–µ –¥—Ä—É–≥–æ–µ —Å—Ä–µ–¥—Å—Ç–≤–æ
2. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∫–∞–∫ `.wav` –∏–ª–∏ `.mp3` —Ñ–∞–π–ª
3. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –µ–≥–æ –Ω–∏–∂–µ ‚Äî —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–∞ –ø—Ä–∏–≤–µ–¥—ë—Ç —Ñ–æ—Ä–º–∞—Ç –∫ –Ω—É–∂–Ω–æ–º—É
""")

uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª (wav/mp3)", type=["wav", "mp3"])
if uploaded_file is not None:
    st.audio(uploaded_file)

    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_wav:
            if uploaded_file.name.endswith(".mp3"):
                sound = AudioSegment.from_file(uploaded_file, format="mp3")
                sound = sound.set_frame_rate(16000).set_channels(1)
                sound.export(tmp_wav.name, format="wav")
            else:
                tmp_wav.write(uploaded_file.read())

            waveform, sr = torchaudio.load(tmp_wav.name)

        # –ú–æ–Ω–æ
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)

        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ 16 –∫–ì—Ü
        if sr != 16000:
            resampler = T.Resample(sr, 16000)
            waveform = resampler(waveform)
            sr = 16000

        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ 1 —Å–µ–∫—É–Ω–¥–µ
        if waveform.shape[1] < 16000:
            pad_len = 16000 - waveform.shape[1]
            waveform = torch.nn.functional.pad(waveform, (0, pad_len))
        else:
            waveform = waveform[:, :16000]

        # –°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞
        mel = T.MelSpectrogram(sr, n_mels=64)(waveform)
        mel = T.AmplitudeToDB()(mel)
        mel = mel.unsqueeze(0)
        mel = torch.nn.functional.pad(mel, (0, max(0, 128 - mel.shape[-1])))
        mel = mel[:, :, :, :128]

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        out = model(mel)
        pred = out.argmax(dim=1).item()
        command = idx2label.get(pred, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")

        st.success(f"‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: **{command}**")

    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
